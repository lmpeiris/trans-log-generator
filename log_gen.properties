#Configurations for trans log generator
#====================================
[DEFAULT]
# ------ TEMPLATE SOURCES AND OUTPUT CONFIGURATIONS -------------

#The type of logs/events we are creating sample data for
event_type_file=unitTest.csv

output_fileprefix=unittest_translog_

output_filesuffix=.log

#outfile location, without the ending /
output_location=target

#Whether to clear previously generated log files 
overwrite_target=1

#This is the mother file of all field list files. Describe the variable format of fields.
#A wierd bug causing format_file variable not  being able to be used in the code. So use as field_formats.csv directly.but do not uncomment this.
format_file=field_formats.csv

#you can divide the records between files 
record_count_per_file=1000
number_of_files=1

#If we are not generating data for a field, but just importing the list from a file.No trailing '/'. File should be named as fieldname.csv
import_data_location=imports

# ------ PERFORMANCE PARAMETERS ---------------------------------

#number of repetitions of records for repeating columns
num_repeat=2

#number of parallel processes to run. Usually the number of CPU cores you intend to use, put "auto" for auto adjust
processes=auto

# enable compression for output files (only if the number of outputfiles is 1). You can use either gzip or pigz for compression tech.
enable_compression=0
compression_tech="gzip"

# limit memory per child process. 0 is for unlimited. Value is in kilobytes.
max_memory_child=0

# Proportion of data generated uniquely when using fake and regex directly in translog definition
# 0.01 means 1% of data is unique
fake_proportion=0.01

# ------ GENERAL CONFIGURATIONS ---------------------------------
# WARNING - auto config. whether advanced libraries (ex: Math::Random ) is installed in system.
lib_ok=1

# Maintain meta data for trans log format files?
meta_data_enabled=0

# Advanced validation checking on generated data, turn on for unit tests
validation_check=1

#python3 command; could be like simply "python3" or something like "/usr/bin/python3.5"
python3_command=python3

#produce generation timing graph for each column
benchmark_plot=1

# offer target directory on port 2121 as FTP
start_ftp_after_process=0

# ------ TRANSLOG DATA / RECORD CONFIGURATIONS -----------------

#datetime format for generated data. The "" enclosing is abosultely necessary. This is overridden if used "customDT"
# see http://strftime.org/ for the format
datetime_format="%Y-%m-%d %H:%M:%S"
date_format="%Y-%m-%d"
time_format="%H:%M:%S"
# note: for microtime, input values in micro seconds, for others, in seconds. This is used my micros and millis
microtime_format="%Y-%m-%d %H:%M:%S.%f"

#trans log output properties, use the ' ' enclosing
log_delimiter='|'

#will add column names as the first line, current implementation is slow, do not use for large files
add_column_names=0

#will add delimiters at start and end of each record
delimiters_at_ends=0

#enable DOS line endings?
dos_line_endings=0

# ------------------ TRANSLOG POST PROCESSING ----------------------------

# ------ post processor specific settings -------#
#number of jobs to divide in to. Higher number requires more resources to be allocated to ppss, while lower number could bog down the operation due to a slow process. However, jobnum >= processes is better.
jobnum=12
#following is used if the file is not given in the command line
http_client_event_file=target/billpay-translog.log.2013-01-23-15-00.log
#compressed log back up location, make log_compress=1 to enable. useful if invoke by automated script
log_compress=0
compressed_logs=ppss_compressed_logs
#whether HTTP success / fail counts are checked
http_success_counts=0

# ----- template post processing --------------- #
#generic template builder, select custom, json_single(all records inside one json object), json_multi (multiple json objects) or xml
template_build_type=json_multi

#enable automatic template (whether to build template or not)
post_template_enable=1

#template source files and outfile
template_file=templates/test.txt
template_out_file=target/out.json

#custom record settings. Select template_build_type as 'custom' to enable. If no header, footer or divider, keep empty.
template_file_header=
template_file_footer=
divider_between_records=,
unique_first_record=1
repeating_tag_xml=Record

# --- matplotlib plotting -----#
plotting_format=bs
#Plot type, scatter for scatter plot (Y vs X) or histogram
plot_type=histogram
#number of bars for histogram
plot_bars=100

# ------ field configuration --------------------------------
# used when generating addresses using shuffle merge
max_address_number=999


# --- async HTTP client -----
# request types: POST (ascii), GET (url encoded values)
http_request_type=POST
http_target_url=http://192.168.1.4:8888
# set tcp_close 1 to create connection per request. This will retain connection pool. tackle TIME_WAITs in server side
# 0 would reuse connections, but will reduce to 1 quiet soon. Tackle by low pool size and high jobnum
tcp_close=0
connection_pool=10
# if 0, then no time out
http_timeout=0
#====================================================================

